{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl\n",
      "  Using cached trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.3-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.13-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting rich (from trl)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-76.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.5.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Using cached trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached bitsandbytes-0.45.3-py3-none-win_amd64.whl (75.4 MB)\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached aiohttp-3.11.13-cp312-cp312-win_amd64.whl (438 kB)\n",
      "Using cached huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached setuptools-76.0.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached propcache-0.3.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, pyarrow, propcache, numpy, networkx, multidict, mdurl, MarkupSafe, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, markdown-it-py, jinja2, aiosignal, torch, rich, huggingface-hub, aiohttp, tokenizers, bitsandbytes, accelerate, transformers, datasets, trl, peft\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.4.0 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 attrs-25.1.0 bitsandbytes-0.45.3 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.3.2 dill-0.3.8 filelock-3.17.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.2 idna-3.10 jinja2-3.1.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.3 pandas-2.2.3 peft-0.14.0 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.5.3 setuptools-76.0.0 sympy-1.13.1 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.49.0 trl-0.15.2 typing-extensions-4.12.2 tzdata-2025.1 urllib3-2.3.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets trl torch accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\fyp\\000 raad\\.venv\\lib\\site-packages (76.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dialogues: 21434\n",
      "Columns: ['npc_role', 'player_input', 'npc_response', 'emotion']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>npc_role</th>\n",
       "      <th>player_input</th>\n",
       "      <th>npc_response</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mechanic</td>\n",
       "      <td>Hey, I heard you're the best when it comes to ...</td>\n",
       "      <td>Absolutely, I know my way around an engine lik...</td>\n",
       "      <td>Confidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mechanic</td>\n",
       "      <td>Hey, I heard you're the best when it comes to ...</td>\n",
       "      <td>No doubt about it. I take pride in delivering ...</td>\n",
       "      <td>Confidence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mechanic</td>\n",
       "      <td>Hey, I heard you're the best when it comes to ...</td>\n",
       "      <td>You can count on me—I've handled tougher engin...</td>\n",
       "      <td>Confidence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   npc_role                                       player_input  \\\n",
       "0  Mechanic  Hey, I heard you're the best when it comes to ...   \n",
       "1  Mechanic  Hey, I heard you're the best when it comes to ...   \n",
       "2  Mechanic  Hey, I heard you're the best when it comes to ...   \n",
       "\n",
       "                                        npc_response     emotion  \n",
       "0  Absolutely, I know my way around an engine lik...  Confidence  \n",
       "1  No doubt about it. I take pride in delivering ...  Confidence  \n",
       "2  You can count on me—I've handled tougher engin...  Confidence  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's saved as 'Dataset.xlsx')\n",
    "df = pd.read_excel(\"Dataset.xlsx\")\n",
    "\n",
    "# Basic dataset info\n",
    "print(f\"Total dialogues: {len(df)}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Preview first few rows\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npc_role\n",
      "Tourist                    3743\n",
      "Mechanic                   3079\n",
      "Office Corporate Worker    3042\n",
      "Security Guard             2669\n",
      "Sweeper                    2346\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['npc_role'].value_counts().head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average player_input length: 9.274610432023888\n",
      "Average npc_response length: 11.670523467388263\n"
     ]
    }
   ],
   "source": [
    "df['player_length'] = df['player_input'].str.split().str.len()\n",
    "df['response_length'] = df['npc_response'].str.split().str.len()\n",
    "print(\"\\nAverage player_input length:\", df['player_length'].mean())\n",
    "print(\"Average npc_response length:\", df['response_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\FYP\\000 Raad\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 examples from the dataset:\n",
      "{'text': '{\"npc_role\": \"Mechanic\", \"player_input\": \"Hey, I heard you\\'re the best when it comes to fixing engines.\", \"emotion\": \"Confidence\", \"npc_response\": \"Absolutely, I know my way around an engine like the back of my hand. Trust me, you\\'re in good hands.\"}'}\n",
      "{'text': '{\"npc_role\": \"Mechanic\", \"player_input\": \"Hey, I heard you\\'re the best when it comes to fixing engines.\", \"emotion\": \"Confidence\", \"npc_response\": \"No doubt about it. I take pride in delivering top-notch work every time.\"}'}\n",
      "{'text': '{\"npc_role\": \"Mechanic\", \"player_input\": \"Hey, I heard you\\'re the best when it comes to fixing engines.\", \"emotion\": \"Confidence\", \"npc_response\": \"You can count on me\\\\u2014I\\'ve handled tougher engines than this one.\"}'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create JSON Schema prompt for each example using the given column names\n",
    "def create_prompt(row):\n",
    "    data = {\n",
    "        \"npc_role\": row[\"npc_role\"],\n",
    "        \"player_input\": row[\"player_input\"],\n",
    "        \"emotion\": row[\"emotion\"],\n",
    "        \"npc_response\": row[\"npc_response\"]\n",
    "    }\n",
    "    return json.dumps(data)\n",
    "\n",
    "df[\"text\"] = df.apply(create_prompt, axis=1)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset (using only the JSON schema text)\n",
    "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
    "print(\"\\nFirst 3 examples from the dataset:\")\n",
    "for i in range(3):\n",
    "    print(dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainable parameters:\n",
      "trainable params: 921,600 || all params: 135,436,608 || trainable%: 0.6805\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model on CPU (no quantization; bitsandbytes is not used)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "# Set up LoRA configuration for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,             # Low-rank dimension (adjustable)\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Attach LoRA adapters to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 21434/21434 [00:01<00:00, 18083.20 examples/s]\n",
      "Filter: 100%|██████████| 21434/21434 [00:01<00:00, 14135.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokenized output:\n",
      "{'input_ids': [39428, 4413, 83, 79, 10305, 1799, 476, 6407, 2410, 286, 1002, 476, 15284, 79, 4525, 1799, 476, 22234, 28, 339, 3984, 346, 2316, 260, 1450, 645, 357, 2216, 288, 21276, 9396, 14069, 476, 391, 6430, 1799, 476, 19168, 1667, 1002, 476, 4413, 83, 79, 7639, 1799, 476, 42686, 8234, 28, 339, 699, 957, 970, 1130, 354, 2327, 702, 260, 1056, 282, 957, 1369, 30, 10306, 549, 28, 346, 2316, 281, 1123, 3288, 1270, 109], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for the JSON schema text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\n",
    "print(\"\\nSample tokenized output:\")\n",
    "print(tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator to pad and create labels for causal LM training\n",
    "def custom_data_collator(features):\n",
    "    batch = tokenizer.pad(features, return_tensors=\"pt\")\n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"].long()\n",
    "    if \"attention_mask\" in batch:\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"].long()\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeed config saved as 'deepspeed_config.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create and save a DeepSpeed configuration file for efficient training\n",
    "deepspeed_config = {\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": True,\n",
    "      \"buffer_count\": 4\n",
    "    },\n",
    "    \"contiguous_gradients\": True,\n",
    "    \"overlap_comm\": False,\n",
    "    \"allgather_partitions\": True,\n",
    "    \"reduce_scatter\": True\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": True\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"deepspeed_config.json\", \"w\") as f:\n",
    "    json.dump(deepspeed_config, f, indent=2)\n",
    "\n",
    "print(\"DeepSpeed config saved as 'deepspeed_config.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3615' max='3615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3615/3615 4:32:11, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.398300</td>\n",
       "      <td>1.233498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.299400</td>\n",
       "      <td>1.167496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.282900</td>\n",
       "      <td>1.147907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./SmolLM2-finetuned\\\\tokenizer_config.json',\n",
       " './SmolLM2-finetuned\\\\special_tokens_map.json',\n",
       " './SmolLM2-finetuned\\\\vocab.json',\n",
       " './SmolLM2-finetuned\\\\merges.txt',\n",
       " './SmolLM2-finetuned\\\\added_tokens.json',\n",
       " './SmolLM2-finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the tokenized dataset into training and validation sets (90% train, 10% eval)\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./SmolLM2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=custom_data_collator,\n",
    ")\n",
    "\n",
    "# Train the model with tqdm progress bars automatically shown by the Trainer,\n",
    "# then save the fine-tuned model and tokenizer.\n",
    "trainer.train()\n",
    "trainer.save_model(\"./SmolLM2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./SmolLM2-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"NPC_Response\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import json\n",
    "\n",
    "# Define a function to generate an NPC response using JSON schema,\n",
    "# extracting only the 'npc_response' field from the output.\n",
    "def generate_npc_response(npc_role: str, player_input: str, emotion: str, generator, max_length: int = 100) -> str:\n",
    "    prompt_dict = {\n",
    "        \"npc_role\": npc_role,\n",
    "        \"player_input\": player_input,\n",
    "        \"emotion\": emotion,\n",
    "        \"npc_response\": \"\"\n",
    "    }\n",
    "    prompt = json.dumps(prompt_dict)\n",
    "    output = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    try:\n",
    "        generated_json = json.loads(generated_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Attempt to extract JSON substring if full decoding fails\n",
    "        start = generated_text.find(\"{\")\n",
    "        end = generated_text.rfind(\"}\") + 1\n",
    "        json_str = generated_text[start:end]\n",
    "        try:\n",
    "            generated_json = json.loads(json_str)\n",
    "        except Exception:\n",
    "            return \"Error in JSON decoding\"\n",
    "    return generated_json.get(\"npc_response\", \"\").strip()\n",
    "\n",
    "# Initialize the text-generation pipeline using the fine-tuned model\n",
    "generator = pipeline(\"text-generation\", model=\"./SmolLM2-finetuned\", tokenizer=\"./SmolLM2-finetuned\")\n",
    "\n",
    "# Generate a sample NPC response and print only the npc_response in JSON format\n",
    "npc_response = generate_npc_response(\n",
    "    npc_role=\"Mechanic\",\n",
    "    player_input=\"My car's engine is making weird sounds; can you help?\",\n",
    "    emotion=\"Confidence\",\n",
    "    generator=generator\n",
    ")\n",
    "print(json.dumps({\"NPC_Response\": npc_response}, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranslate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleu_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      4\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Evaluate model performance using perplexity and BLEU score\n",
    "\n",
    "# 1. Compute Perplexity using the Trainer's evaluation (ensure your evaluation dataset is set up)\n",
    "eval_results = trainer.evaluate()\n",
    "eval_loss = eval_results.get(\"eval_loss\", eval_results.get(\"loss\"))\n",
    "perplexity = math.exp(eval_loss)\n",
    "print(\"\\nValidation Perplexity:\", perplexity)\n",
    "\n",
    "# 2. Compute average BLEU score on 10 samples from the original dataset\n",
    "bleu_scores = []\n",
    "for i in range(10):\n",
    "    sample = json.loads(dataset[i][\"text\"])\n",
    "    prompt_dict = {\n",
    "        \"npc_role\": sample[\"npc_role\"],\n",
    "        \"player_input\": sample[\"player_input\"],\n",
    "        \"emotion\": sample[\"emotion\"],\n",
    "        \"npc_response\": \"\"\n",
    "    }\n",
    "    prompt = json.dumps(prompt_dict)\n",
    "    output = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    try:\n",
    "        generated_json = json.loads(generated_text)\n",
    "    except:\n",
    "        start = generated_text.find(\"{\")\n",
    "        end = generated_text.rfind(\"}\") + 1\n",
    "        json_str = generated_text[start:end]\n",
    "        try:\n",
    "            generated_json = json.loads(json_str)\n",
    "        except:\n",
    "            generated_json = {\"npc_response\": \"\"}\n",
    "    reference = sample[\"npc_response\"].split() \n",
    "    hypothesis = generated_json.get(\"npc_response\", \"\").split()\n",
    "    bleu = sentence_bleu([reference], hypothesis)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(\"Average BLEU score on 10 samples:\", avg_bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
